{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1NNgmrgzoFY8SAXFBEgSipXsiYiYHTPvQ","timestamp":1695357788091}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/uzh-rpg/rpg_timelens/blob/main/TimeLens.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"s4PPH-NTx3dF"},"source":["# TimeLens: Event-based Video Frame Interpolation"]},{"cell_type":"markdown","metadata":{"id":"9PnK9tknyDza"},"source":["## Installation\n","\n","First clone the repo and install an environment"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kg_xBex5yJZx","outputId":"e2347bcc-d525-45c9-976e-60c0a69a13a5","executionInfo":{"status":"ok","timestamp":1695356442337,"user_tz":-540,"elapsed":10,"user":{"displayName":"조pride","userId":"18100762325334682086"}}},"source":["# Check your current GPU\n","# If you are lucky, you get 16GB VRAM. If you are not lucky, you get less. VRAM is important. The more VRAM, the higher the maximum resolution will go\n","!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fri Sep 22 04:20:41 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   69C    P8    13W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"markdown","metadata":{"id":"XUtaNkg0GEt4"},"source":["# Connect Google Drive"]},{"cell_type":"code","metadata":{"id":"CTGU8anrzVh-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d3864b3d-abe5-42ca-a488-e2a6838383cb","executionInfo":{"status":"ok","timestamp":1695356504945,"user_tz":-540,"elapsed":62613,"user":{"displayName":"조pride","userId":"18100762325334682086"}}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","print('Google Drive connected.')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","Google Drive connected.\n"]}]},{"cell_type":"code","metadata":{"id":"lQ_4dtx9Fb1v","executionInfo":{"status":"ok","timestamp":1695356504945,"user_tz":-540,"elapsed":16,"user":{"displayName":"조pride","userId":"18100762325334682086"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8cfe43f1-8682-4d8a-a17d-f1871481977d"},"source":["!ls /content/gdrive\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MyDrive\n"]}]},{"cell_type":"code","source":["%%bash\n","rm -rf /content/rpg_timelens\n","rm -rf /content/ontent\n","rm -rf /content/sample_data\n","rm -rf /content/example"],"metadata":{"id":"xmvhHoP3tgqz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bNvjLMgM1siz"},"source":["# Install Dependencies and Repo"]},{"cell_type":"code","metadata":{"id":"qSmCBWoy1l_E","executionInfo":{"status":"ok","timestamp":1695356554994,"user_tz":-540,"elapsed":50058,"user":{"displayName":"조pride","userId":"18100762325334682086"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"12da52d7-e33e-4b0d-ef13-de47d19d1158"},"source":["%cd /content\n","# !git clone https://github.com/uzh-rpg/rpg_timelens\n","# !git clone https://github.com/Chohoonhee/rpg_timelens\n","!git clone https://github.com/AI-Expert-Project/rpg_timelens\n","%cd /content/rpg_timelens\n","# !wget http://rpg.ifi.uzh.ch/timelens/data2/checkpoint.bin\n","!wget --load-cookies ~/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies ~/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1O43qlJMvmKQLMKlXlp42V7_Ww1r7sWio' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1O43qlJMvmKQLMKlXlp42V7_Ww1r7sWio\" -O checkpoint.bin && rm -rf ~/cookies.txt\n","\n","!CUDA_VISIBLE_DEVICES=0"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","Cloning into 'rpg_timelens'...\n","remote: Enumerating objects: 146, done.\u001b[K\n","remote: Counting objects: 100% (43/43), done.\u001b[K\n","remote: Compressing objects: 100% (27/27), done.\u001b[K\n","remote: Total 146 (delta 28), reused 16 (delta 16), pack-reused 103\u001b[K\n","Receiving objects: 100% (146/146), 73.59 MiB | 1.65 MiB/s, done.\n","Resolving deltas: 100% (57/57), done.\n","/content/rpg_timelens\n","--2023-09-22 04:22:31--  https://docs.google.com/uc?export=download&confirm=t&id=1O43qlJMvmKQLMKlXlp42V7_Ww1r7sWio\n","Resolving docs.google.com (docs.google.com)... 142.251.10.100, 142.251.10.102, 142.251.10.138, ...\n","Connecting to docs.google.com (docs.google.com)|142.251.10.100|:443... connected.\n","HTTP request sent, awaiting response... 303 See Other\n","Location: https://doc-00-a8-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/l3uik9ar367cei7hrth8a3gmo06rmlmc/1695356550000/18100762325334682086/*/1O43qlJMvmKQLMKlXlp42V7_Ww1r7sWio?e=download&uuid=e930dc10-1845-4b8b-b657-4e85e2ddb0e8 [following]\n","Warning: wildcards not supported in HTTP.\n","--2023-09-22 04:22:32--  https://doc-00-a8-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/l3uik9ar367cei7hrth8a3gmo06rmlmc/1695356550000/18100762325334682086/*/1O43qlJMvmKQLMKlXlp42V7_Ww1r7sWio?e=download&uuid=e930dc10-1845-4b8b-b657-4e85e2ddb0e8\n","Resolving doc-00-a8-docs.googleusercontent.com (doc-00-a8-docs.googleusercontent.com)... 74.125.200.132, 2404:6800:4003:c00::84\n","Connecting to doc-00-a8-docs.googleusercontent.com (doc-00-a8-docs.googleusercontent.com)|74.125.200.132|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 475379363 (453M) [application/x-zip]\n","Saving to: ‘checkpoint.bin’\n","\n","checkpoint.bin      100%[===================>] 453.36M   186MB/s    in 2.4s    \n","\n","2023-09-22 04:22:34 (186 MB/s) - ‘checkpoint.bin’ saved [475379363/475379363]\n","\n"]}]},{"cell_type":"code","metadata":{"id":"QuRNCO7Z2bAn"},"source":["################# Required Configurations ############################\n","\n","#@markdown # Required Configuration\n","#@markdown Use the values in here to configure what you'd like TimeLens to do. If you do not have your own data, just leave the values below and it will download an example.\n","\n","#@markdown ## Output file\n","#@markdown Output file path: path (relative to the root of your Google Drive) for the output file. It will also determine the filetype in the destination. `.mp4` is recommended for video input, `.gif` for gif inputs.\n","OUTPUT_FILE_PATH = \"/content/rpg_timelens/out.mp4\" #@param{type:\"string\"}\n","\n","################# Optional configurations ############################\n","\n","#@markdown # Optional Configuration\n","#@markdown Parameters below can be left with their defaults, but feel free to adapt them to your needs.\n","\n","#@markdown ## Target FPS\n","#@markdown  how many frames per second should the result have. This will determine how many intermediate images are interpolated.\n","TARGET_FPS =  960#@param{type:\"number\"}\n","\n","#@markdown ## Frame input directory\n","#@markdown A path, relative to your GDrive root, where you already have the list of frames in the format 00001.png, 00002.png, etc.\n","FRAME_INPUT_DIR = '/content/rpg_timelens/input_frames' #@param{type:\"string\"}\n","EVENTS_INPUT_DIR = '/content/rpg_timelens/input_events' #@param{type:\"string\"}\n","\n","#@markdown ## Frame output directory\n","#@markdown A path, relative to your GDrive root, where you want the generated frame.\n","FRAME_OUTPUT_DIR = '/content/rpg_timelens/output_frames' #@param{type:\"string\"}\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fZJHZ2T4Hdgu"},"source":["# Set Up Example\n","If you do not have data to process, just leave the default values in the form above and the cell below will download an example."]},{"cell_type":"code","metadata":{"id":"418W2cPS2fla","executionInfo":{"status":"ok","timestamp":1695356774650,"user_tz":-540,"elapsed":5871,"user":{"displayName":"조pride","userId":"18100762325334682086"}},"outputId":"0f606a4a-70c5-4952-a3e9-23e7a16b5db9","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Example\n","\n","%%bash\n","wget http://rpg.ifi.uzh.ch/timelens/data2/example_github.zip -O /content/example_github.zip\n","unzip /content/example_github.zip -d /content/\n","rm -rf /content/example_github.zip\n","mv /content/example/events /content/rpg_timelens/input_events\n","mv /content/example/images /content/rpg_timelens/input_frames\n","mkdir /content/rpg_timelens/output_frames\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Process is interrupted.\n"]}]},{"cell_type":"code","metadata":{"id":"R_e_NEoO9Erq","executionInfo":{"status":"error","timestamp":1695356775020,"user_tz":-540,"elapsed":1,"user":{"displayName":"조pride","userId":"18100762325334682086"}},"outputId":"57553dde-ad49-44cb-9467-4af594d8575a","colab":{"base_uri":"https://localhost:8080/","height":410}},"source":["# find the fps of the input images\n","import numpy as np\n","from os.path import join\n","\n","timestamps = np.genfromtxt(join(FRAME_INPUT_DIR, \"timestamp.txt\"))\n","fps = int(1000000.0 / np.diff(timestamps).mean())\n","factor = int(TARGET_FPS / fps)\n","inserts = factor-1\n","print(\"Upsampling from \", fps, \" to \", TARGET_FPS, \" with \", inserts , \" inserts.\")\n","\n"],"execution_count":null,"outputs":[{"ename":"FileNotFoundError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-1899c324df05>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtimestamps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFRAME_INPUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"timestamp.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mfps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000000.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestamps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mfactor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTARGET_FPS\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mgenfromtxt\u001b[0;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, ndmin, like)\u001b[0m\n\u001b[1;32m   1957\u001b[0m         \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1958\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1959\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1960\u001b[0m         \u001b[0mfid_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1961\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    531\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    532\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{path} not found.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: /content/rpg_timelens/input_frames/timestamp.txt not found."]}]},{"cell_type":"markdown","metadata":{"id":"wKK8kUG5mnLL"},"source":["# TimeLens Networks"]},{"cell_type":"markdown","source":["<img src = \"https://drive.google.com/uc?id=10DOjPpDBfODA4rjjWew5EVyzfoBW58Mx\" height = 300 width = 900>"],"metadata":{"id":"10i9vu8c8yMl"}},{"cell_type":"markdown","source":["<img src = \"https://drive.google.com/uc?id=1aMlvxCFf8qq7Zz4UNdNJM3wBZCec5bs_\" height = 300 width = 1100>\n"],"metadata":{"id":"ZHcSFyQ--lfb"}},{"cell_type":"code","source":["import os\n","import sys\n","\n","import pkg_resources\n","\n","import torch as th\n","import torch.nn.functional as F\n","from timelens.common import size_adapter\n","from torch import nn\n","\n","class down(nn.Module):\n","    def __init__(self, inChannels, outChannels, filterSize):\n","        super(down, self).__init__()\n","        self.conv1 = nn.Conv2d(\n","            inChannels,\n","            outChannels,\n","            filterSize,\n","            stride=1,\n","            padding=int((filterSize - 1) / 2),\n","        )\n","        self.conv2 = nn.Conv2d(\n","            outChannels,\n","            outChannels,\n","            filterSize,\n","            stride=1,\n","            padding=int((filterSize - 1) / 2),\n","        )\n","\n","    def forward(self, x):\n","        x = F.avg_pool2d(x, 2)\n","        x = F.leaky_relu(self.conv1(x), negative_slope=0.1)\n","        x = F.leaky_relu(self.conv2(x), negative_slope=0.1)\n","        return x\n","\n","class up(nn.Module):\n","    def __init__(self, inChannels, outChannels):\n","        super(up, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels=?, out_channels=?, kernel_size=? , stride=?, padding=?)\n","        self.conv2 = nn.Conv2d(in_channels=?, out_channels=?, kernel_size=? , stride=?, padding=?)\n","\n","    def forward(self, x, skpCn):\n","        x = F.interpolate(x, scale_factor=2, mode=\"bilinear\")\n","\n","        x = F.leaky_relu(?, negative_slope=0.1)\n","        x = F.leaky_relu(?, negative_slope=0.1) # concatenation\n","        return x\n","\n","\n","class UNet(nn.Module):\n","    \"\"\"Modified version of Unet from SuperSloMo.\n","\n","    Difference :\n","    1) there is an option to skip ReLU after the last convolution.\n","    2) there is a size adapter module that makes sure that input of all sizes\n","       can be processed correctly. It is necessary because original\n","       UNet can process only inputs with spatial dimensions divisible by 32.\n","    \"\"\"\n","\n","    def __init__(self, inChannels, outChannels, ends_with_relu=True):\n","        super(UNet, self).__init__()\n","        self._ends_with_relu = ends_with_relu\n","        self._size_adapter = size_adapter.SizeAdapter(minimum_size=32)\n","        self.conv1 = nn.Conv2d(?, ?, ?, stride=?, padding=?)\n","        self.conv2 = nn.Conv2d(?, ?, ?, stride=?, padding=?)\n","        self.down1 = down(?, ?, ?)\n","        self.down2 = down(?, ?, ?)\n","        self.down3 = down(?, ?, ?)\n","        self.down4 = down(?, ?, ?)\n","        self.down5 = down(?, ?, ?)\n","        self.up1 = up(?, ?)\n","        self.up2 = up(?, ?)\n","        self.up3 = up(?, ?)\n","        self.up4 = up(?, ?)\n","        self.up5 = up(?,?)\n","        self.conv3 = nn.Conv2d(?, ?, ?, stride=?, padding=?)\n","\n","    def forward(self, x):\n","        # Size adapter spatially augments input to the size divisible by 32.\n","        x = self._size_adapter.pad(x)\n","        x = F.leaky_relu(self.conv1(x), negative_slope=0.1)\n","        s1 = F.leaky_relu(self.conv2(x), negative_slope=0.1)\n","        s2 = self.down1(s1)\n","        s3 = self.down2(s2)\n","        s4 = self.down3(s3)\n","        s5 = self.down4(s4)\n","        x = self.down5(s5)\n","        x = self.up1(x, s5)\n","        x = self.up2(x, s4)\n","        x = self.up3(x, s3)\n","        x = self.up4(x, s2)\n","        x = self.up5(x, s1)\n","\n","        # Note that original code has relu et the end.\n","        if self._ends_with_relu == True:\n","            x = F.leaky_relu(self.conv3(x), negative_slope=0.1)\n","        else:\n","            x = self.conv3(x)\n","        # Size adapter crops the output to the original size.\n","        x = self._size_adapter.unpad(x)\n","        return x\n"],"metadata":{"id":"kIDJuJqOnGI3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<img src = \"https://drive.google.com/uc?id=10DOjPpDBfODA4rjjWew5EVyzfoBW58Mx\" height = 300 width = 900>"],"metadata":{"id":"p79_DTRYC1SN"}},{"cell_type":"code","source":["import torch as th\n","from timelens.common import warp\n","from timelens.common import pytorch_tools\n","from torch import nn\n","\n","\n","# ----------------------------------Warping-based Interpolation-----------------------------------------------\n","def _pack_voxel_grid_for_flow_estimation(example):\n","    return th.cat(\n","        [example[\"before\"][\"reversed_voxel_grid\"], example[\"after\"][\"voxel_grid\"]]\n","    )\n","\n","def _pack_images_for_warping(example):\n","    return th.cat(\n","        [example[\"before\"][\"rgb_image_tensor\"], example[\"after\"][\"rgb_image_tensor\"]]\n","    )\n","\n","class Warp(nn.Module):\n","    def __init__(self):\n","        super(Warp, self).__init__()\n","        self.flow_network = UNet(?, ?, False)\n","\n","    def from_legacy_checkpoint(self, checkpoint_filename):\n","        checkpoint = th.load(checkpoint_filename)\n","        self.load_state_dict(checkpoint[\"networks\"])\n","\n","    def run_warp(self, example):\n","        flow = self.flow_network(_pack_voxel_grid_for_flow_estimation(example))\n","        warped, warped_invalid = warp.backwarp_2d(\n","            source=_pack_images_for_warping(example),\n","            y_displacement=flow[:, 0, ...],\n","            x_displacement=flow[:, 1, ...],\n","        )\n","        (before_flow, after_flow) = th.chunk(flow, chunks=2)\n","        (before_warped, after_warped) = th.chunk(warped, chunks=2)\n","        (before_warped_invalid, after_warped_invalid) = th.chunk(\n","            warped_invalid.detach(), chunks=2\n","        )\n","        return (\n","            before_warped,\n","            after_warped,\n","            before_flow,\n","            after_flow,\n","            before_warped_invalid,\n","            after_warped_invalid,\n","        )\n","\n","    def run_and_pack_to_example(self, example):\n","        _pack_output_to_example(example, self.run_warp(example))\n","\n","    def forward(self, example):\n","        return self.run_warp(example)\n","# ----------------------------------Interpolation Synthesis-----------------------------------------------\n","def _pack(example):\n","    return th.cat([example['before']['voxel_grid'],\n","                   example['before']['rgb_image_tensor'],\n","                   example['after']['voxel_grid'],\n","                   example['after']['rgb_image_tensor']], dim=1)\n","\n","class Fusion(nn.Module):\n","    def __init__(self):\n","        super(Fusion, self).__init__()\n","        self.fusion_network = UNet(? + ?, ?, False)\n","\n","    def run_fusion(self, example):\n","        return self.fusion_network(_pack(example))\n","\n","    def from_legacy_checkpoint(self, checkpoint_filename):\n","        checkpoint = th.load(checkpoint_filename)\n","        self.load_state_dict(checkpoint[\"networks\"])\n","\n","    def run_and_pack_to_example(self, example):\n","        example['middle']['fusion'] = self.run_fusion(example)\n","\n","    def forward(self, example):\n","        return self.run_fusion(example)\n","\n","\n","# ----------------------------------Warping Refinement-----------------------------------------------\n","\n","def _pack_for_residual_flow_computation(example):\n","    tensors = [\n","        example[\"middle\"][\"{}_warped\".format(packet)] for packet in [\"after\", \"before\"]\n","    ]\n","    tensors.append(example[\"middle\"][\"fusion\"])\n","    return th.cat(tensors, dim=1)\n","\n","\n","def _pack_images_for_second_warping(example):\n","    return th.cat(\n","        [example[\"middle\"][\"after_warped\"], example[\"middle\"][\"before_warped\"]],\n","    )\n","\n","def _pack_output_to_example(example, output):\n","    (\n","        example[\"middle\"][\"before_refined_warped\"],\n","        example[\"middle\"][\"after_refined_warped\"],\n","        example[\"middle\"][\"before_refined_warped_invalid\"],\n","        example[\"middle\"][\"after_refined_warped_invalid\"],\n","        example[\"before\"][\"residual_flow\"],\n","        example[\"after\"][\"residual_flow\"],\n","    ) = output\n","\n","\n","class RefineWarp(Warp, Fusion):\n","    def __init__(self):\n","        Warp.__init__(self)\n","        self.flow_refinement_network = UNet(?, ?, False)\n","\n","    def run_refine_warp(self, example):\n","        Warp.run_and_pack_to_example(self, example)\n","        Fusion.run_and_pack_to_example(self, example)\n","        residual = self.flow_refinement_network(\n","            _pack_for_residual_flow_computation(example)\n","        )\n","        (after_residual, before_residual) = th.chunk(residual, 2, dim=1)\n","        residual = th.cat([after_residual, before_residual], dim=0)\n","        refined, refined_invalid = warp.backwarp_2d(\n","            source=_pack_images_for_second_warping(example),\n","            y_displacement=residual[:, 0, ...],\n","            x_displacement=residual[:, 1, ...],\n","        )\n","\n","        (after_refined, before_refined) = th.chunk(refined, 2)\n","        (after_refined_invalid, before_refined_invalid) = th.chunk(\n","            refined_invalid.detach(), 2)\n","        return (\n","            before_refined,\n","            after_refined,\n","            before_refined_invalid,\n","            after_refined_invalid,\n","            before_residual,\n","            after_residual,\n","        )\n","\n","\n","    def run_fast(self, example):\n","        Warp.run_and_pack_to_example(self, example)\n","        Fusion.run_and_pack_to_example(self, example)\n","        residual = self.flow_refinement_network(\n","            _pack_for_residual_flow_computation(example)\n","        )\n","        (after_residual, before_residual) = th.chunk(residual, 2, dim=1)\n","        residual = th.cat([after_residual, before_residual], dim=0)\n","        refined, _ = warp.backwarp_2d(\n","            source=_pack_images_for_second_warping(example),\n","            y_displacement=residual[:, 0, ...],\n","            x_displacement=residual[:, 1, ...],\n","        )\n","\n","        return th.chunk(refined, 2)\n","\n","    def run_and_pack_to_example(self, example):\n","        _pack_output_to_example(example, self.run_refine_warp(example))\n","\n","    def forward(self, example):\n","        return self.run_refine_warp(example)\n","\n","\n","# -------------------------------------Attention-based averaging -----------------------------------------\n","def _pack_input_for_attention_computation(example):\n","    fusion = example[\"middle\"][\"fusion\"]\n","    number_of_examples, _, height, width = fusion.size()\n","    return th.cat(\n","        [\n","            example[\"after\"][\"flow\"],\n","            example[\"middle\"][\"after_refined_warped\"],\n","            example[\"before\"][\"flow\"],\n","            example[\"middle\"][\"before_refined_warped\"],\n","            example[\"middle\"][\"fusion\"],\n","            th.Tensor(example[\"middle\"][\"weight\"])\n","            .view(-1, 1, 1, 1)\n","            .expand(number_of_examples, 1, height, width)\n","            .type(fusion.type()),\n","        ],\n","        dim=1,\n","    )\n","\n","\n","def _compute_weighted_average(attention, before_refined, after_refined, fusion):\n","    return (\n","        attention[:, 0, ...].unsqueeze(1) * before_refined\n","        + attention[:, 1, ...].unsqueeze(1) * after_refined\n","        + attention[:, 2, ...].unsqueeze(1) * fusion\n","    )\n","\n","\n","class AttentionAverage(RefineWarp):\n","    def __init__(self):\n","        Warp.__init__(self)\n","        self.attention_network = UNet(?, ?, False)\n","\n","    def run_fast(self, example):\n","        example['middle']['before_refined_warped'],\n","        example['middle']['after_refined_warped'] = RefineWarp.run_fast(self, example)\n","\n","        attention_scores = self.attention_network(\n","            _pack_input_for_attention_computation(example)\n","        )\n","        attention = F.softmax(attention_scores, dim=1)\n","        average = _compute_weighted_average(\n","            attention,\n","            example['middle']['before_refined_warped'],\n","            example['middle']['after_refined_warped'],\n","            example['middle']['fusion']\n","        )\n","        return average, attention\n","\n","    def run_attention_averaging(self, example):\n","        RefineWarp.run_and_pack_to_example(self, example)\n","        attention_scores = self.attention_network(\n","            _pack_input_for_attention_computation(example)\n","        )\n","        attention = F.softmax(attention_scores, dim=1)\n","        average = _compute_weighted_average(\n","            attention,\n","            example[\"middle\"][\"before_refined_warped\"],\n","            example[\"middle\"][\"after_refined_warped\"],\n","            example[\"middle\"][\"fusion\"],\n","        )\n","        return average, attention\n","\n","    def run_and_pack_to_example(self, example):\n","        (\n","            example[\"middle\"][\"attention_average\"],\n","            example[\"middle\"][\"attention\"],\n","        ) = self.run_attention_averaging(example)\n","\n","    def forward(self, example):\n","        return self.run_attention_averaging(example)\n"],"metadata":{"id":"_l4geG6onLWo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZBWMSweHSKUv"},"source":["# Run TimeLens"]},{"cell_type":"code","metadata":{"id":"7NisgZgq-jLN"},"source":["%cd /content/rpg_timelens/\n","!mkdir $FRAME_OUTPUT_DIR\n","!python -m timelens.run_timelens checkpoint.bin $EVENTS_INPUT_DIR $FRAME_INPUT_DIR $FRAME_OUTPUT_DIR 0 $inserts"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MEZEm8rq-uvV"},"source":["!ffmpeg -i \"$FRAME_OUTPUT_DIR\"/%06d.png timelens.mp4\n","from google.colab import files\n","files.download(\"timelens.mp4\")"],"execution_count":null,"outputs":[]}]}